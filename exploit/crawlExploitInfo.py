""" 

crawlExploitInfo.py
====================

Thie module is for gathering extra informations
related to collected exploit codes.

Todo:
  * progress print (using thread & state)
"""

import json
import requests
import subprocess
from bs4 import BeautifulSoup

PERM_OUTPUT_PATH = "/opt/output/perm/"
SEARCHSPLOIT_PATH = "/opt/exploitdb/"
NVD_URL = "https://nvd.nist.gov/vuln/detail/CVE-"
EDB_URL = "https://www.exploit-db.com/exploits/"

# for firewall pypass
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64) \
    AppleWebKit/537.36 (KHTML, like Gecko) \
    Chrome/55.0.2919.83 Safari/537.36'
}

def from_exploitdb(eid):
    """crawling CVE-ID, published date from given exploit-db url.

    Args:
        eid(str): exploit id from searchsploit

    Returns:
        cve(list): CVE-ID of eid
        pub(str): published date of eid

    Raises:
        None: There is no CVE-ID
    
    """
    _html = requests.get(f"{EDB_URL}{eid}", headers= HEADERS)
    _soup = BeautifulSoup(_html.content, 'html.parser')
    found = _soup.find_all(attrs={'class':'stats-title'})    # found = [0:EID, 1:CVE-ID, 2:Author, 3:Type, 4:Platform, 5:Date]

    # CVE-ID
    cve = []
    if not len(found[1]) <= 3: # more than 1 CVE-ID
        found_list = found[1].get_text().strip().split('\n')
        for l in found_list:
            if not l.strip() == "": cve.append(l.strip())
    else:   # only 1 CVE-ID
        cve += [found[1].get_text().strip()] if not "N/A" in found[1].get_text().strip() else [None] # no CVE-ID handling
        if [''] == cve: cve = [None]    # no CVE-ID handling

    # Published Date
    pub = found[5].get_text().strip()

    return cve, pub


def from_nvd(cve):
    """crawling CVSS v2/v3 score from given CVE-ID.

    Args:
        cve(list): CVE-ID of exploit code

    Returns:
        v2(list): CVSS v2 score of CVE-ID
        v3(list): CVSS v3 score of CVE-ID

    Raises:
        None: there is no score.
    """
    if cve == [None]: return None, None # handling no CVE-ID

    v2, v3 = [], []
    for _cve in cve:
        _html = requests.get(f"{NVD_URL}{_cve}", headers= HEADERS)
        _soup = BeautifulSoup(_html.content, 'html.parser')

        found = _soup.find(attrs={'id':'Cvss2CalculatorAnchor'})
        v2 += [found.get_text().split()[0]] if found else [None] 

        found = _soup.find(attrs={'id':'Cvss3NistCalculatorAnchor'})
        v3 += [found.get_text().split()[0]] if found else [None]

    return v2, v3


def crawl_more_info():
    """crawling extra information from websites.

    Note:
        * Use exploit.json file in output directory
        * Save the result in the same json file in output directory
    """
    with open(f'{PERM_OUTPUT_PATH}exploit.json','r') as f:
        jsonList = json.load(f)
        jsonList.pop(-1)
    with open(f'{PERM_OUTPUT_PATH}exploit.json','w') as f:
        for exploitJson in jsonList:
            exploitJson["CVE-ID"], exploitJson["publish"] = from_exploitdb(exploitJson["EID"])
            exploitJson["CVSS"]["CVSSv2"], exploitJson["CVSS"]["CVSSv3"] = from_nvd(exploitJson["CVE-ID"])
        jsonStr = json.dumps(jsonList)
        f.write(jsonStr)


def parsing_kernel_version(desc):
    """parsing related kernel verison from description text of exploit.

    Args:
        desc(str): description text of exploit

    Returns:
        version(str): related kernel version

    Note:
        There is 3 version format
        1) x.x.x
        2) < x.x.x
        3) x.x.x < y.y.y
    """
    descList = desc.split()
    if "Kernel" in descList:
        kIdx = descList.index("Kernel")
    elif "kernel" in descList:
        kIdx = descList.index("kernel")

    if "<" in descList:
        if descList.index("<") == kIdx + 1 or descList.index("<") == kIdx + 2:  # filetering other position of '<'
            if descList.index("<") == 2:    # Linux Kernel < x.x.x
                version = ' '.join(descList[kIdx+1:kIdx+3])
            else:                           # Linux Kernel x.x.x < x.x.x
                version = ' '.join(descList[kIdx+1:kIdx+4])
        else:   # no '<'
            version = descList[kIdx+1]
    else:   # no '<'
        version = descList[kIdx+1]

    return version


def searchsploit_info():
    """collecting additional informations from searchsploit.

    Note:
        * Use files_exploits.csv for collecting
        * Save the result in json format in output directory
    """
    # search "linux kernel", "privilege escalation" keyword in Description and "local" in Type 
    cmd = f'cat {SEARCHSPLOIT_PATH}files_exploits.csv | grep -i "Linux Kernel" | grep -i "Privilege Escalation" | grep -i ",local"'
    searchsploit_result = subprocess.check_output(cmd,shell=True).decode().strip("\n").split('\n')
    searchsploit_result = list(map(lambda x: x.split(','), searchsploit_result))

    jsonList = list()
    with open(f'{PERM_OUTPUT_PATH}exploit.json','w') as f:
        for i, result in enumerate(searchsploit_result):
            # check whether c file or not
            if not ".c" in result[1]:
                continue
            # Initialization
            exploitJson = {"EID":"", "publish":"", "CVE-ID":"", "src":"", "url":"", "CVSS":{"CVSSv2":"","CVSSv3":""}, "kernel version":""}
            exploitJson["EID"]             = result[0]
            exploitJson["src"]             = "exploitdb"
            exploitJson["url"]             = "exploit-db.com/exploits/"+result[0]
            exploitJson["kernel version"]  = parsing_kernel_version(result[2])
            jsonList.append(exploitJson)

        jsonStr = json.dumps(jsonList)
        f.write(jsonStr)

if __name__ == "__main__":
    searchsploit_info()
    crawl_more_info()